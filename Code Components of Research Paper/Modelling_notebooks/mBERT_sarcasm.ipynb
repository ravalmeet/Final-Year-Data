{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7517697,"sourceType":"datasetVersion","datasetId":4378989}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-31T16:50:45.797594Z","iopub.execute_input":"2024-01-31T16:50:45.797947Z","iopub.status.idle":"2024-01-31T16:50:46.941587Z","shell.execute_reply.started":"2024-01-31T16:50:45.797918Z","shell.execute_reply":"2024-01-31T16:50:46.940541Z"},"trusted":true,"id":"pr3kA3C_5YtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","import torch\n","\n","# Step 1: Load CSV dataset\n","dataset_path = '/content/updated_hindi.csv'\n","df = pd.read_csv(dataset_path)\n","\n"],"metadata":{"execution":{"iopub.status.busy":"2024-01-31T16:50:46.943421Z","iopub.execute_input":"2024-01-31T16:50:46.944223Z","iopub.status.idle":"2024-01-31T16:50:54.943793Z","shell.execute_reply.started":"2024-01-31T16:50:46.944187Z","shell.execute_reply":"2024-01-31T16:50:54.942961Z"},"trusted":true,"id":"bTfpyJ055YtD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.dropna(inplace=True)"],"metadata":{"execution":{"iopub.status.busy":"2024-01-31T05:00:49.417487Z","iopub.execute_input":"2024-01-31T05:00:49.418459Z","iopub.status.idle":"2024-01-31T05:00:49.438713Z","shell.execute_reply.started":"2024-01-31T05:00:49.418421Z","shell.execute_reply":"2024-01-31T05:00:49.437927Z"},"trusted":true,"id":"iezAGeiB5YtE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names=['1','0']"],"metadata":{"id":"jyvWFKSAlSSp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'bert-base-multilingual-cased'"],"metadata":{"id":"dPcjfeOQlcb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the Config object, with an output configured for classification.\n","config = AutoConfig.from_pretrained(pretrained_model_name_or_path=model_name,\n","                                    num_labels=len(class_names))\n","\n","print('Config type:', str(type(config)), '\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52cv8bqFlRsL","outputId":"d4b6dbdc-ef50-4063-f300-25304cc0a360"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config type: <class 'transformers.models.bert.configuration_bert.BertConfig'> \n","\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoModel , AutoModelForSequenceClassification, AutoConfig , AutoTokenizer , AdamW\n","import torch\n","from tqdm import tqdm\n","\n","\n","# Load the BERT tokenizer.\n","print(f'Loading {model_name} tokenizer...')\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n","\n","\n","class SarcasmDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n","\n","        input_ids = encoding['input_ids'].flatten()\n","        attention_mask = encoding['attention_mask'].flatten()\n","\n","        return {\n","            'text': text,\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Convert labels to numerical format\n","label_mapping = {label: idx for idx, label in enumerate(df['result'].unique())}\n","df['result'] = df['result'].map(label_mapping)\n","\n","# Split the dataset\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","train_dataset = SarcasmDataset(train_df['text'].tolist(), train_df['result'].tolist(), tokenizer, max_len=128)\n","val_dataset = SarcasmDataset(val_df['text'].tolist(), val_df['result'].tolist(), tokenizer, max_len=128)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","\n","# Step 4: Fine-tuning\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name,config = config)\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # This is the value Michael used.\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","\n","num_epochs = 2\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    train_true_labels = []\n","    train_pred_labels = []\n","\n","    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        texts = batch['text']\n","\n","        # for text, label, input_id, attn_mask in zip(texts, labels, input_ids, attention_mask):\n","        #   print(f\"Text: {text}\")\n","        #   print(f\"Label: {label}\")\n","          # print(f\"Input IDs: {input_id}\")\n","          # print(f\"Attention Mask: {attn_mask}\")\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Record true labels and predicted labels for accuracy calculation\n","        train_true_labels.extend(labels.cpu().numpy())\n","        train_pred_labels.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n","\n","        total_loss += loss.item()\n","\n","    # Calculate accuracy for the epoch\n","    epoch_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n","\n","    # Print average loss and accuracy for the epoch\n","    average_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Training Loss: {average_loss:.4f}, Training Accuracy: {epoch_accuracy * 100:.2f}%\")\n","\n","# Step 5: Evaluation\n","model.eval()\n","all_labels = []\n","all_predictions = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(val_loader, desc='Evaluating'):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n","\n","        all_labels.extend(labels.cpu().numpy())\n","        all_predictions.extend(predictions)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(all_labels, all_predictions)\n","print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-01-31T17:53:47.665826Z","iopub.execute_input":"2024-01-31T17:53:47.666567Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"mI25loKh5YtE","outputId":"d60ef901-dca2-4000-fce7-d6205dceea6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading bert-base-multilingual-cased tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1/2: 100%|██████████| 1600/1600 [05:46<00:00,  4.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2, Average Training Loss: 0.1129, Training Accuracy: 95.98%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/2: 100%|██████████| 1600/1600 [05:44<00:00,  4.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/2, Average Training Loss: 0.0465, Training Accuracy: 98.56%\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 400/400 [00:24<00:00, 16.46it/s]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 98.12%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"leLh4_415YtE"},"execution_count":null,"outputs":[]}]}