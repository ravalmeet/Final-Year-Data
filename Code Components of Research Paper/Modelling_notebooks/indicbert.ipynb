{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7316626,"sourceType":"datasetVersion","datasetId":4245834},{"sourceId":7374304,"sourceType":"datasetVersion","datasetId":4284777},{"sourceId":7472773,"sourceType":"datasetVersion","datasetId":4350497}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-31T13:44:26.483075Z","iopub.execute_input":"2024-01-31T13:44:26.483391Z","iopub.status.idle":"2024-01-31T13:44:26.837044Z","shell.execute_reply.started":"2024-01-31T13:44:26.483361Z","shell.execute_reply":"2024-01-31T13:44:26.836170Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/combinedgbmp/combined.csv\n/kaggle/input/finaldataset/final_dataset_rp.csv\n/kaggle/input/gujaratif/final_gujarati (1).csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport torch\n\n# Step 1: Load CSV dataset\ndataset_path = '//kaggle/input/finaldataset/final_dataset_rp.csv'\ndf = pd.read_csv(dataset_path)\n\n# Step 2: Preprocess the Data\ntokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\nmax_length = 128  # You can adjust this based on your data and model requirements\n\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n        return {'input_ids': encoding['input_ids'].squeeze(), 'attention_mask': encoding['attention_mask'].squeeze(), 'labels': torch.tensor(label)}\n\n# Convert labels to numerical format\nlabel_mapping = {label: idx for idx, label in enumerate(df['result'].unique())}\ndf['result'] = df['result'].map(label_mapping)\n\n# Split the dataset\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain_dataset = CustomDataset(train_df['text'].tolist(), train_df['result'].tolist())\ntest_dataset = CustomDataset(test_df['text'].tolist(), test_df['result'].tolist())\n","metadata":{"execution":{"iopub.status.busy":"2024-01-31T13:44:46.900689Z","iopub.execute_input":"2024-01-31T13:44:46.901600Z","iopub.status.idle":"2024-01-31T13:44:57.419275Z","shell.execute_reply.started":"2024-01-31T13:44:46.901562Z","shell.execute_reply":"2024-01-31T13:44:57.418203Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c9250d4a0a4e5986df549492ebf52a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db50491e1081452295972c841635e512"}},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ntokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n\nclass SarcasmDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n            truncation=True\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        return {\n            'text': text,\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Convert labels to numerical format\nlabel_mapping = {label: idx for idx, label in enumerate(df['result'].unique())}\ndf['result'] = df['result'].map(label_mapping)\n\n# Split the dataset\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain_dataset = SarcasmDataset(train_df['text'].tolist(), train_df['result'].tolist(), tokenizer, max_len=128)\nval_dataset = SarcasmDataset(val_df['text'].tolist(), val_df['result'].tolist(), tokenizer, max_len=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n# Step 4: Fine-tuning\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForSequenceClassification.from_pretrained('ai4bharat/indic-bert', num_labels=len(label_mapping))\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    train_true_labels = []\n    train_pred_labels = []\n\n    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        # Record true labels and predicted labels for accuracy calculation\n        train_true_labels.extend(labels.cpu().numpy())\n        train_pred_labels.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n\n        total_loss += loss.item()\n\n    # Calculate accuracy for the epoch\n    epoch_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n\n    # Print average loss and accuracy for the epoch\n    average_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Training Loss: {average_loss:.4f}, Training Accuracy: {epoch_accuracy * 100:.2f}%\")\n\n# Step 5: Evaluation\nmodel.eval()\nall_labels = []\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in tqdm(val_loader, desc='Evaluating'):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n\n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predictions)\n\n# Calculate accuracy\naccuracy = accuracy_score(all_labels, all_predictions)\nprint(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-31T13:55:28.170576Z","iopub.execute_input":"2024-01-31T13:55:28.170942Z","iopub.status.idle":"2024-01-31T14:39:01.481745Z","shell.execute_reply.started":"2024-01-31T13:55:28.170916Z","shell.execute_reply":"2024-01-31T14:39:01.480790Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1/10: 100%|██████████| 1600/1600 [04:20<00:00,  6.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Average Training Loss: 0.2356, Training Accuracy: 90.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 1600/1600 [04:19<00:00,  6.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Average Training Loss: 0.1159, Training Accuracy: 95.99%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 1600/1600 [04:19<00:00,  6.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Average Training Loss: 0.1166, Training Accuracy: 95.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 1600/1600 [04:19<00:00,  6.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Average Training Loss: 0.1041, Training Accuracy: 96.29%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 1600/1600 [04:18<00:00,  6.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Average Training Loss: 0.1392, Training Accuracy: 95.16%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 1600/1600 [04:18<00:00,  6.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Average Training Loss: 0.0906, Training Accuracy: 97.02%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 1600/1600 [04:18<00:00,  6.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Average Training Loss: 0.0593, Training Accuracy: 98.07%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 1600/1600 [04:17<00:00,  6.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Average Training Loss: 0.0561, Training Accuracy: 98.30%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 1600/1600 [04:17<00:00,  6.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Average Training Loss: 0.0508, Training Accuracy: 98.48%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 1600/1600 [04:17<00:00,  6.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Average Training Loss: 0.0337, Training Accuracy: 98.95%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 400/400 [00:22<00:00, 17.97it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 97.44%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}